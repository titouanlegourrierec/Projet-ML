{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   file      label\n",
      "0     chest_xray/train/NORMAL/NORMAL2-IM-0927-0001.jpeg     normal\n",
      "1     chest_xray/train/NORMAL/NORMAL2-IM-1056-0001.jpeg     normal\n",
      "2             chest_xray/train/NORMAL/IM-0427-0001.jpeg     normal\n",
      "3     chest_xray/train/NORMAL/NORMAL2-IM-1260-0001.jpeg     normal\n",
      "4        chest_xray/train/NORMAL/IM-0656-0001-0001.jpeg     normal\n",
      "...                                                 ...        ...\n",
      "5213  chest_xray/train/PNEUMONIA/person142_virus_288...  pneumonia\n",
      "5214  chest_xray/train/PNEUMONIA/person364_bacteria_...  pneumonia\n",
      "5215  chest_xray/train/PNEUMONIA/person1323_virus_22...  pneumonia\n",
      "5216  chest_xray/train/PNEUMONIA/person772_virus_140...  pneumonia\n",
      "5217  chest_xray/train/PNEUMONIA/person501_virus_101...  pneumonia\n",
      "\n",
      "[5218 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the directory paths\n",
    "normal_dir = 'chest_xray/train/NORMAL'\n",
    "pneumonia_dir = 'chest_xray/train/PNEUMONIA'\n",
    "\n",
    "# Get the list of all files in each directory\n",
    "normal_files = [os.path.join(normal_dir, file) for file in os.listdir(normal_dir)]\n",
    "pneumonia_files = [os.path.join(pneumonia_dir, file) for file in os.listdir(pneumonia_dir)]\n",
    "\n",
    "# Create a DataFrame\n",
    "train_df = pd.DataFrame({\n",
    "    'file': normal_files + pneumonia_files,\n",
    "    'label': ['normal'] * len(normal_files) + ['pneumonia'] * len(pneumonia_files)\n",
    "})\n",
    "\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  file      label\n",
      "0             chest_xray/test/NORMAL/IM-0031-0001.jpeg     normal\n",
      "1             chest_xray/test/NORMAL/IM-0025-0001.jpeg     normal\n",
      "2     chest_xray/test/NORMAL/NORMAL2-IM-0272-0001.jpeg     normal\n",
      "3     chest_xray/test/NORMAL/NORMAL2-IM-0102-0001.jpeg     normal\n",
      "4     chest_xray/test/NORMAL/NORMAL2-IM-0229-0001.jpeg     normal\n",
      "..                                                 ...        ...\n",
      "619  chest_xray/test/PNEUMONIA/person120_bacteria_5...  pneumonia\n",
      "620  chest_xray/test/PNEUMONIA/person171_bacteria_8...  pneumonia\n",
      "621  chest_xray/test/PNEUMONIA/person109_bacteria_5...  pneumonia\n",
      "622  chest_xray/test/PNEUMONIA/person83_bacteria_41...  pneumonia\n",
      "623  chest_xray/test/PNEUMONIA/person112_bacteria_5...  pneumonia\n",
      "\n",
      "[624 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the directory paths\n",
    "normal_dir = 'chest_xray/test/NORMAL'\n",
    "pneumonia_dir = 'chest_xray/test/PNEUMONIA'\n",
    "\n",
    "# Get the list of all files in each directory\n",
    "normal_files = [os.path.join(normal_dir, file) for file in os.listdir(normal_dir)]\n",
    "pneumonia_files = [os.path.join(pneumonia_dir, file) for file in os.listdir(pneumonia_dir)]\n",
    "\n",
    "# Create a DataFrame\n",
    "test_df = pd.DataFrame({\n",
    "    'file': normal_files + pneumonia_files,\n",
    "    'label': ['normal'] * len(normal_files) + ['pneumonia'] * len(pneumonia_files)\n",
    "})\n",
    "\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 file      label\n",
      "0     chest_xray/val/NORMAL/NORMAL2-IM-1440-0001.jpeg     normal\n",
      "1     chest_xray/val/NORMAL/NORMAL2-IM-1437-0001.jpeg     normal\n",
      "2                     chest_xray/val/NORMAL/.DS_Store     normal\n",
      "3     chest_xray/val/NORMAL/NORMAL2-IM-1431-0001.jpeg     normal\n",
      "4     chest_xray/val/NORMAL/NORMAL2-IM-1436-0001.jpeg     normal\n",
      "5     chest_xray/val/NORMAL/NORMAL2-IM-1430-0001.jpeg     normal\n",
      "6     chest_xray/val/NORMAL/NORMAL2-IM-1438-0001.jpeg     normal\n",
      "7     chest_xray/val/NORMAL/NORMAL2-IM-1442-0001.jpeg     normal\n",
      "8     chest_xray/val/NORMAL/NORMAL2-IM-1427-0001.jpeg     normal\n",
      "9                  chest_xray/val/PNEUMONIA/.DS_Store  pneumonia\n",
      "10  chest_xray/val/PNEUMONIA/person1950_bacteria_4...  pneumonia\n",
      "11  chest_xray/val/PNEUMONIA/person1951_bacteria_4...  pneumonia\n",
      "12  chest_xray/val/PNEUMONIA/person1952_bacteria_4...  pneumonia\n",
      "13  chest_xray/val/PNEUMONIA/person1946_bacteria_4...  pneumonia\n",
      "14  chest_xray/val/PNEUMONIA/person1947_bacteria_4...  pneumonia\n",
      "15  chest_xray/val/PNEUMONIA/person1946_bacteria_4...  pneumonia\n",
      "16  chest_xray/val/PNEUMONIA/person1949_bacteria_4...  pneumonia\n",
      "17  chest_xray/val/PNEUMONIA/person1954_bacteria_4...  pneumonia\n"
     ]
    }
   ],
   "source": [
    "# Define the directory paths\n",
    "normal_dir = 'chest_xray/val/NORMAL'\n",
    "pneumonia_dir = 'chest_xray/val/PNEUMONIA'\n",
    "\n",
    "# Get the list of all files in each directory\n",
    "normal_files = [os.path.join(normal_dir, file) for file in os.listdir(normal_dir)]\n",
    "pneumonia_files = [os.path.join(pneumonia_dir, file) for file in os.listdir(pneumonia_dir)]\n",
    "\n",
    "# Create a DataFrame\n",
    "val_df = pd.DataFrame({\n",
    "    'file': normal_files + pneumonia_files,\n",
    "    'label': ['normal'] * len(normal_files) + ['pneumonia'] * len(pneumonia_files)\n",
    "})\n",
    "\n",
    "print(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Générateur des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 validated image filenames belonging to 2 classes.\n",
      "Found 16 validated image filenames belonging to 2 classes.\n",
      "Found 624 validated image filenames belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/titouanlegourrierec/anaconda3/lib/python3.11/site-packages/keras/src/preprocessing/image.py:1137: UserWarning: Found 2 invalid image filename(s) in x_col=\"file\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "/Users/titouanlegourrierec/anaconda3/lib/python3.11/site-packages/keras/src/preprocessing/image.py:1137: UserWarning: Found 2 invalid image filename(s) in x_col=\"file\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255.,  # scale pixel values to [0, 1]\n",
    "    shear_range=0.2,  # randomly apply shearing transformations\n",
    "    zoom_range=0.2,  # randomly zooming inside pictures\n",
    "    horizontal_flip=True  # randomly flip half of the images horizontally\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col=\"file\",\n",
    "    y_col=\"label\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    x_col=\"file\",\n",
    "    y_col=\"label\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')\n",
    "\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col=\"file\",\n",
    "    y_col=\"label\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 28s - loss: 0.6646 - accuracy: 0.7559 - val_loss: 1.5724 - val_accuracy: 0.5000 - 28s/epoch - 278ms/step\n",
      "Epoch 2/15\n",
      "100/100 - 28s - loss: 0.3611 - accuracy: 0.8503 - 28s/epoch - 277ms/step\n",
      "Epoch 3/15\n",
      "100/100 - 28s - loss: 0.2772 - accuracy: 0.8913 - 28s/epoch - 275ms/step\n",
      "Epoch 4/15\n",
      "100/100 - 29s - loss: 0.2510 - accuracy: 0.8972 - 29s/epoch - 288ms/step\n",
      "Epoch 5/15\n",
      "100/100 - 29s - loss: 0.2283 - accuracy: 0.9103 - 29s/epoch - 295ms/step\n",
      "Epoch 6/15\n",
      "100/100 - 29s - loss: 0.1952 - accuracy: 0.9209 - 29s/epoch - 291ms/step\n",
      "Epoch 7/15\n",
      "100/100 - 29s - loss: 0.1827 - accuracy: 0.9297 - 29s/epoch - 290ms/step\n",
      "Epoch 8/15\n",
      "100/100 - 29s - loss: 0.1825 - accuracy: 0.9319 - 29s/epoch - 287ms/step\n",
      "Epoch 9/15\n",
      "100/100 - 29s - loss: 0.1717 - accuracy: 0.9325 - 29s/epoch - 287ms/step\n",
      "Epoch 10/15\n",
      "100/100 - 30s - loss: 0.1622 - accuracy: 0.9409 - 30s/epoch - 297ms/step\n",
      "Epoch 11/15\n",
      "100/100 - 30s - loss: 0.1433 - accuracy: 0.9466 - 30s/epoch - 298ms/step\n",
      "Epoch 12/15\n",
      "100/100 - 29s - loss: 0.1394 - accuracy: 0.9444 - 29s/epoch - 293ms/step\n",
      "Epoch 13/15\n",
      "100/100 - 29s - loss: 0.1541 - accuracy: 0.9419 - 29s/epoch - 293ms/step\n",
      "Epoch 14/15\n",
      "100/100 - 29s - loss: 0.1510 - accuracy: 0.9381 - 29s/epoch - 292ms/step\n",
      "Epoch 15/15\n",
      "100/100 - 29s - loss: 0.1310 - accuracy: 0.9538 - 29s/epoch - 294ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,  # nombre d'images = batch_size * steps\n",
    "      epochs=15,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,  # nombre d'images = batch_size * steps\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 4s 221ms/step - loss: 0.5625 - accuracy: 0.7821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5625074505805969, 0.7820512652397156]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(\"hello world\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
